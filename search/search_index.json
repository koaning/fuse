{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Sentence Encoder Diaries My employer Rasa (we're hiring) gives me education days each year that I can use to teach myself new skills. In 2020 I used this time to challenge myself to make a different kind of educational content. Now, in 2021 I decided to use these days to build a sentence encoder from scratch. The goal isn't to build something that is very state of the art, but rather to build something that is useful while learning lots of things along the way. The project The goal is to create a fine-tunable sentence encoder. Hence the name fusebox . It's a box of tricks related to this encoder. Hopefully, the end product is something that will help me label a new dataset, but the main goal is to learn interesting things while working on a hard problem.","title":"Home"},{"location":"#the-sentence-encoder-diaries","text":"My employer Rasa (we're hiring) gives me education days each year that I can use to teach myself new skills. In 2020 I used this time to challenge myself to make a different kind of educational content. Now, in 2021 I decided to use these days to build a sentence encoder from scratch. The goal isn't to build something that is very state of the art, but rather to build something that is useful while learning lots of things along the way.","title":"The Sentence Encoder Diaries"},{"location":"#the-project","text":"The goal is to create a fine-tunable sentence encoder. Hence the name fusebox . It's a box of tricks related to this encoder. Hopefully, the end product is something that will help me label a new dataset, but the main goal is to learn interesting things while working on a hard problem.","title":"The project"},{"location":"chapter-01/","text":"Day 01: The Big Picture How the ball got rolling. Tools like word2vec and BERT have proven to be useful. You didn't need labelled data because you could just use the word-order as a pattern to learn. It idea became very popular and it turned out to be a reasonable pre-processing technique. I was wondering if these systems are really training a signal that I'm interested in though. Both systems don't really learn the meaning of a word, they merely predict in what order they are likely to appear. There's a proxy in there that's useful, but it's also incredibly hard to steer since the system has no actual notion of the meaning of a word. It's a preprocessing technique that's very easy to apply, but very hard to trust. An Example Let's show a quick demo from spaCy. import spacy nlp = spacy . load ( \"en_core_web_md\" ) doc1 = nlp ( \"I love python.\" ) doc2 = nlp ( \"I hate python.\" ) # Similarity of two documents print ( doc1 , \"<->\" , doc2 , doc1 . similarity ( doc2 )) # Output: # I love python. <-> I hate python. 0.951918946265527 The sentence \"I love python\" seems to be very similar to \"I hate python\". This seems strange, but it's what you can expect from a system that's trained to predict where words might appear in a sentence. The word \"hate\" and \"love\" might have an opposite meaning, but you can easily exchange them and the sentence would still be grammatically solid. That's why there's a high \"similarity\" score attached. There's a lot of information in these pretrained embeddings, but it might not be the information that you're interested in. What else can we do? If I were training a system to detect sentiment in a specific domain, let's say sentiment on a particular website, then I'm more interested in a pre-trained system that has been trained on other sentiment tasks in the online domain. That's much better than a system that can predict the next word on Wikipedia. What I'm after, I think, is a system that looks more like this: Overview of an architecture. The idea is to have a system that is able to learn on multiple tasks. Each task represents a labelled dataset that contains a specific signal that you're interested in. By adding many of them you may just get a very general embedding that's able to handle tasks that are similar to the main task that you're interested in. In the case of online sentiment, you can try to fetch other online sentiment datasets as a proxy. Multiple tasks will push the embeddings in different directions. What's especially nice about this architecture is that it should be relatively easy to, after finetuning, add the main task that you're interested in. It is, after all, just another task. Adding a main task, to finetune towards, after pre-training is done. Benefits There's a lot about this architecture diagram that I like. We seem to be very flexible in the tasks t hat we add. We can add classification tasks, tagging tasks or even question answering tasks. To keep things simple though, I'll only consider architectures that embed an entire sentence that ignores word order. That means that I won't handle entity detection tasks for now. Another big benefit is that we're also very flexible in terms of datasets that we add. If you're interested in twitter analytics, you can simply only add tasks that do some sort of classification on twitter. A big downside of using big pretrained BERT/word2vec models is that they overfit on Wikipedia-esque datasources, which may be totally unlike your use-case. You're also able to be more selective in the datasets that you add. You may want to add many small datasets with labels that you trust instead of hoping that a 3rd party cares about label quality. It's also good to point out that we're still able to add a task that tries to predict the next word in a sequence. That would also just be another task. Finally, another thing that I like about this approach is that we can try to keep things lightweight. I'll likely use keras for this project, but by keeping the embedding layers at bay as well as the tasks I should be able to build something that can train/predict on commodity hardware. Open Questions There's plenty of open questions that I'll need to tackle though. How do you train on multiple tasks effectively? How can I make sure that we don't forget what we learned from a batch of data from task 1 by the time that we reach task X? Can I prevent that we overfit on tasks that have a larger dataset attached? Can we do something clever with the tokeniser? It'd be nice if we can easily swap parts so that we can rely on subword tokens so that we're robust against spelling. It'd also be grand if we could use a trick that prevents our vocabulary from blowing up (hint: there is one). If we want to keep this system lightweight we cannot use the .fit().predict() -API since it will require us to have all the data in memory. Instead, we may want to fall back to the .fit_partial().predict() API that's also commonly use in vowpal wabbit and spaCy . What architecture decisions will make sense when we're interested in training embeddings? During the pre-training phase we may care more about a generally useful representation than an accuracy number. But this won't be the case when we actually go to our fine-tunable tasks. How can we keep everything lightweight? On my end it's a hard constraint that this system needs to be trainable on a CPU first and a GPU second. Can we really trust labels in our tasks? Generally, the answer is no . Instead of resorting to blind trust, it'd also be nice if we had a system that does some active learning and could tell us when there's some examples in a tasks that deserve double-checking. Can we prevent big grids? Grid-Search tends to be a distraction, especially when grids get big . I'd love to spend less time doing grid-search and more time investigating my data. It'd therefore be very preferable if we came up with something that's relatively general and doesn't require the tuning of many parameters. It's fine if we allow customisation, but it needs to be clear upfront what the effect of a change might be. Google once made a system that does pretty much what I'm describing called the \"Universal Sentence Encoder\" . They released pretrained models, but they never released the system that allows you to train your own. One could wonder, is there a good reason for this? It may be very hard to train such a system. Up Next I'm pretty excited about this work. It feels both like a great idea, while also feeling like I have no idea what I'm doing . In a good way. This combination usually leads me to learn many things. I hope it will be interesting to other folks too. So I'll add a chapter whenever I reach a new milestone.","title":"Day 01: The Big Picture"},{"location":"chapter-01/#day-01-the-big-picture","text":"","title":"Day 01: The Big Picture"},{"location":"chapter-01/#how-the-ball-got-rolling","text":"Tools like word2vec and BERT have proven to be useful. You didn't need labelled data because you could just use the word-order as a pattern to learn. It idea became very popular and it turned out to be a reasonable pre-processing technique. I was wondering if these systems are really training a signal that I'm interested in though. Both systems don't really learn the meaning of a word, they merely predict in what order they are likely to appear. There's a proxy in there that's useful, but it's also incredibly hard to steer since the system has no actual notion of the meaning of a word. It's a preprocessing technique that's very easy to apply, but very hard to trust.","title":"How the ball got rolling."},{"location":"chapter-01/#an-example","text":"Let's show a quick demo from spaCy. import spacy nlp = spacy . load ( \"en_core_web_md\" ) doc1 = nlp ( \"I love python.\" ) doc2 = nlp ( \"I hate python.\" ) # Similarity of two documents print ( doc1 , \"<->\" , doc2 , doc1 . similarity ( doc2 )) # Output: # I love python. <-> I hate python. 0.951918946265527 The sentence \"I love python\" seems to be very similar to \"I hate python\". This seems strange, but it's what you can expect from a system that's trained to predict where words might appear in a sentence. The word \"hate\" and \"love\" might have an opposite meaning, but you can easily exchange them and the sentence would still be grammatically solid. That's why there's a high \"similarity\" score attached. There's a lot of information in these pretrained embeddings, but it might not be the information that you're interested in.","title":"An Example"},{"location":"chapter-01/#what-else-can-we-do","text":"If I were training a system to detect sentiment in a specific domain, let's say sentiment on a particular website, then I'm more interested in a pre-trained system that has been trained on other sentiment tasks in the online domain. That's much better than a system that can predict the next word on Wikipedia. What I'm after, I think, is a system that looks more like this: Overview of an architecture. The idea is to have a system that is able to learn on multiple tasks. Each task represents a labelled dataset that contains a specific signal that you're interested in. By adding many of them you may just get a very general embedding that's able to handle tasks that are similar to the main task that you're interested in. In the case of online sentiment, you can try to fetch other online sentiment datasets as a proxy. Multiple tasks will push the embeddings in different directions. What's especially nice about this architecture is that it should be relatively easy to, after finetuning, add the main task that you're interested in. It is, after all, just another task. Adding a main task, to finetune towards, after pre-training is done.","title":"What else can we do?"},{"location":"chapter-01/#benefits","text":"There's a lot about this architecture diagram that I like. We seem to be very flexible in the tasks t hat we add. We can add classification tasks, tagging tasks or even question answering tasks. To keep things simple though, I'll only consider architectures that embed an entire sentence that ignores word order. That means that I won't handle entity detection tasks for now. Another big benefit is that we're also very flexible in terms of datasets that we add. If you're interested in twitter analytics, you can simply only add tasks that do some sort of classification on twitter. A big downside of using big pretrained BERT/word2vec models is that they overfit on Wikipedia-esque datasources, which may be totally unlike your use-case. You're also able to be more selective in the datasets that you add. You may want to add many small datasets with labels that you trust instead of hoping that a 3rd party cares about label quality. It's also good to point out that we're still able to add a task that tries to predict the next word in a sequence. That would also just be another task. Finally, another thing that I like about this approach is that we can try to keep things lightweight. I'll likely use keras for this project, but by keeping the embedding layers at bay as well as the tasks I should be able to build something that can train/predict on commodity hardware.","title":"Benefits"},{"location":"chapter-01/#open-questions","text":"There's plenty of open questions that I'll need to tackle though. How do you train on multiple tasks effectively? How can I make sure that we don't forget what we learned from a batch of data from task 1 by the time that we reach task X? Can I prevent that we overfit on tasks that have a larger dataset attached? Can we do something clever with the tokeniser? It'd be nice if we can easily swap parts so that we can rely on subword tokens so that we're robust against spelling. It'd also be grand if we could use a trick that prevents our vocabulary from blowing up (hint: there is one). If we want to keep this system lightweight we cannot use the .fit().predict() -API since it will require us to have all the data in memory. Instead, we may want to fall back to the .fit_partial().predict() API that's also commonly use in vowpal wabbit and spaCy . What architecture decisions will make sense when we're interested in training embeddings? During the pre-training phase we may care more about a generally useful representation than an accuracy number. But this won't be the case when we actually go to our fine-tunable tasks. How can we keep everything lightweight? On my end it's a hard constraint that this system needs to be trainable on a CPU first and a GPU second. Can we really trust labels in our tasks? Generally, the answer is no . Instead of resorting to blind trust, it'd also be nice if we had a system that does some active learning and could tell us when there's some examples in a tasks that deserve double-checking. Can we prevent big grids? Grid-Search tends to be a distraction, especially when grids get big . I'd love to spend less time doing grid-search and more time investigating my data. It'd therefore be very preferable if we came up with something that's relatively general and doesn't require the tuning of many parameters. It's fine if we allow customisation, but it needs to be clear upfront what the effect of a change might be. Google once made a system that does pretty much what I'm describing called the \"Universal Sentence Encoder\" . They released pretrained models, but they never released the system that allows you to train your own. One could wonder, is there a good reason for this? It may be very hard to train such a system.","title":"Open Questions"},{"location":"chapter-01/#up-next","text":"I'm pretty excited about this work. It feels both like a great idea, while also feeling like I have no idea what I'm doing . In a good way. This combination usually leads me to learn many things. I hope it will be interesting to other folks too. So I'll add a chapter whenever I reach a new milestone.","title":"Up Next"},{"location":"chapter-02/","text":"Day 02: The Final Layer In this document I'd like to discuss the logic that went into designing the final layer of my architecture. That is to say, the layer that connects the embedding output to each task. Overview of the architecture idea. How do Blobs Become Blobs? Before explaining what I'd like to do in the final layer of my neural network I'd like to motivate it with a drawing. Let's say that we're interested in getting embeddings that have clustered points that are similar. We'd maybe like to have something like below. Simple two dimensional blobs. But how might we get embeddings with such a shape? If we had a \"logistic regression\"-style layer that connects the embedding layer with a task then we may get an embedded space like below. Decision boundaries and associated points. My drawing is likely an exaggeration but it's showing that we can classify to our hearts content without actually generating blobs of clusters. Instead, we may be generating points that are merely linearly separated. If we're interested in creating an embedded space, do we really want to construct it via a final layer that learns labels by separating planes? Maybe it'd be better if we tried to associate a label with an embedding as well. Maybe we can embed the labels too. When we embed the labels in the same space at the text we're still able to learn a classifier, but it'd be based on the distance (or similarity) between the label embedding and the text embedding. That means that every label from every task would be represented in the same space as the text and will be pushing and pulling text embeddings into appropriate clusters. The StarSpace Trick What I'm describing here is what I like to call \"the starspace trick\". It's a trick that's described in the StarSpace: Embed All The Things! -paper which I've seen work very well in Rasa's DIET and TED algorithms. If you're interested in a intuitive deep dive, you may enjoy this video that I made on behalf of Rasa but the short story is as follows: You make an embedding for the text but you also create an embedding of the same size for each of the labels. You can calculate a similarity between the text embedding and each of the label embeddings. This similarity score can be normalised so that you end up with scores for classification. What's cool about this is that your embedded space won't just contain text embeddings. It will also contain label embeddings! This adds a little bit of extra interpretability to the space. What's especially cool in our case; we will embed many labels from different tasks! That allows us to varify if we see patterns that we'd expect. So let's re-draw our architecture diagrams for different tasks. Classification Tasks Labels are encoded as embeddings too. The idea is to take our text embedding and to compare it against a label embedding. The embedding for the correct label needs to have a high similarity while the embeddings for the incorrect labels should all have a low similary. That's the pattern that our network should learn. If we get that right, we've got a classifier that's using the StarSpace trick. Tagging Tasks Tags can be encoded as one-off embeddings. Classification imples that there is only one correct class per text. Many tasks fit this scenario, but it doesn't really fit the \"tagging\" usecase. You could also associate a single text with many tags. For example, a piece of text may contain both \"gratitude\" and \"relief\" at the same time. To handle the \"tagging\" use-case we can make a small variation on the classification use-case. We simply take each possible tag, and each tag is treated as a binary classification problem. Text Comparison Tasks Top might be a question, bottom could be an answer. After thinking about it, we may also use this trick to associate texts with eachother. If we take a question/answering dataset we can try to predict if a certain question is answered by a certain answer. We wouldn't encode a label anymore, but we would again be able to compare two embeddings by using \"similarity\" again. We can sample examples of question/answer pairs that are not related (call these negative samples). The would need to have a low similarity while the actual question/answer pair should have a high similarity. This trick isn't limited to question/answer pairs though. We may also encode text in a conversation between two people with this route. A positive label would indicate two texts happen in sequence while negative labels would indicate they don't. Details to Figure Out I've explained here why I'll be building the architecture in a certain way but I want to acknolwedge there's lots of numeric details to get right. To keep things simple I'll likely focus on the classification/tagging tasks first before I consider the text comparison tasks.","title":"Day 02: The Final Layer"},{"location":"chapter-02/#day-02-the-final-layer","text":"In this document I'd like to discuss the logic that went into designing the final layer of my architecture. That is to say, the layer that connects the embedding output to each task. Overview of the architecture idea.","title":"Day 02: The Final Layer"},{"location":"chapter-02/#how-do-blobs-become-blobs","text":"Before explaining what I'd like to do in the final layer of my neural network I'd like to motivate it with a drawing. Let's say that we're interested in getting embeddings that have clustered points that are similar. We'd maybe like to have something like below. Simple two dimensional blobs. But how might we get embeddings with such a shape? If we had a \"logistic regression\"-style layer that connects the embedding layer with a task then we may get an embedded space like below. Decision boundaries and associated points. My drawing is likely an exaggeration but it's showing that we can classify to our hearts content without actually generating blobs of clusters. Instead, we may be generating points that are merely linearly separated. If we're interested in creating an embedded space, do we really want to construct it via a final layer that learns labels by separating planes? Maybe it'd be better if we tried to associate a label with an embedding as well. Maybe we can embed the labels too. When we embed the labels in the same space at the text we're still able to learn a classifier, but it'd be based on the distance (or similarity) between the label embedding and the text embedding. That means that every label from every task would be represented in the same space as the text and will be pushing and pulling text embeddings into appropriate clusters.","title":"How do Blobs Become Blobs?"},{"location":"chapter-02/#the-starspace-trick","text":"What I'm describing here is what I like to call \"the starspace trick\". It's a trick that's described in the StarSpace: Embed All The Things! -paper which I've seen work very well in Rasa's DIET and TED algorithms. If you're interested in a intuitive deep dive, you may enjoy this video that I made on behalf of Rasa but the short story is as follows: You make an embedding for the text but you also create an embedding of the same size for each of the labels. You can calculate a similarity between the text embedding and each of the label embeddings. This similarity score can be normalised so that you end up with scores for classification. What's cool about this is that your embedded space won't just contain text embeddings. It will also contain label embeddings! This adds a little bit of extra interpretability to the space. What's especially cool in our case; we will embed many labels from different tasks! That allows us to varify if we see patterns that we'd expect. So let's re-draw our architecture diagrams for different tasks.","title":"The StarSpace Trick"},{"location":"chapter-02/#classification-tasks","text":"Labels are encoded as embeddings too. The idea is to take our text embedding and to compare it against a label embedding. The embedding for the correct label needs to have a high similarity while the embeddings for the incorrect labels should all have a low similary. That's the pattern that our network should learn. If we get that right, we've got a classifier that's using the StarSpace trick.","title":"Classification Tasks"},{"location":"chapter-02/#tagging-tasks","text":"Tags can be encoded as one-off embeddings. Classification imples that there is only one correct class per text. Many tasks fit this scenario, but it doesn't really fit the \"tagging\" usecase. You could also associate a single text with many tags. For example, a piece of text may contain both \"gratitude\" and \"relief\" at the same time. To handle the \"tagging\" use-case we can make a small variation on the classification use-case. We simply take each possible tag, and each tag is treated as a binary classification problem.","title":"Tagging Tasks"},{"location":"chapter-02/#text-comparison-tasks","text":"Top might be a question, bottom could be an answer. After thinking about it, we may also use this trick to associate texts with eachother. If we take a question/answering dataset we can try to predict if a certain question is answered by a certain answer. We wouldn't encode a label anymore, but we would again be able to compare two embeddings by using \"similarity\" again. We can sample examples of question/answer pairs that are not related (call these negative samples). The would need to have a low similarity while the actual question/answer pair should have a high similarity. This trick isn't limited to question/answer pairs though. We may also encode text in a conversation between two people with this route. A positive label would indicate two texts happen in sequence while negative labels would indicate they don't.","title":"Text Comparison Tasks"},{"location":"chapter-02/#details-to-figure-out","text":"I've explained here why I'll be building the architecture in a certain way but I want to acknolwedge there's lots of numeric details to get right. To keep things simple I'll likely focus on the classification/tagging tasks first before I consider the text comparison tasks.","title":"Details to Figure Out"}]}