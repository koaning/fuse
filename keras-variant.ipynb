{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6e29d7-0fb5-4cbe-9a14-ae5e349969d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tokenwiser.pipeline import make_partial_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b49d1266-68f3-4efa-93cc-32efb39cb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, name='silicone', subset='dyda_da', split='train', n_feat=20_000):\n",
    "        self.dataset = load_dataset(name, subset)\n",
    "        if isinstance(self.dataset, DatasetDict):\n",
    "            self.dataset = self.dataset[split]\n",
    "        self.labels = list(set(i['Label'] for i in self.dataset))\n",
    "        self.name = f\"{name}-{subset}-{split}\"\n",
    "        self.tfm = tfm = make_partial_union(\n",
    "            HashingVectorizer(n_features=n_feat), \n",
    "            HashingVectorizer(n_features=n_feat, ngram_range=(2, 2))\n",
    "        )\n",
    "        self.label_enc = OneHotEncoder(sparse=False).fit(np.array(self.labels).reshape(-1, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return item['Utterance'], item['Label']\n",
    "    \n",
    "    def batch(self, n):\n",
    "        indices = np.random.randint(len(self), size=n)\n",
    "        texts, labels = zip(*[self[int(i)] for i in indices])\n",
    "        X = self.tfm.transform(texts)\n",
    "        y = self.label_enc.transform(np.array(labels).reshape(-1, 1))\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41537dc7-b317-4fd5-8415-9cec9422443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset silicone (/home/vincent/.cache/huggingface/datasets/silicone/dyda_da/1.0.0/af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5)\n"
     ]
    }
   ],
   "source": [
    "data = TextDataset('silicone', 'dyda_da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f431f3a-f79a-4b77-a8d6-a29a6b913915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87170"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456dbdd-71f5-42b1-9992-d066e926adb4",
   "metadata": {},
   "source": [
    "```python\n",
    "fuse = (\n",
    "    FUSE(tokeniser, n_tok_feat)\n",
    "      .add_task(name, subset)\n",
    "      .add_task(name, subset)\n",
    "      .add_task(name, subset)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8d689b-e542-48b8-9764-18c32d6181e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "X, y = data.batch(1)\n",
    "inputs = Input(shape=(X.shape[1],), sparse=True)\n",
    "emb1 = Dense(256, activation='relu')(inputs)\n",
    "emb2 = Dense(256, activation='relu')(emb1)\n",
    "\n",
    "outputs = Dense(y.shape[1], activation='softmax')(emb2)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0b7884-eac3-4ae9-9557-3a9f0e9ccfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/Development/fuse/venv/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_1/dense_3/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_1/dense_3/embedding_lookup_sparse/Reshape:0\", shape=(None, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_1/dense_3/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/32 [..............................] - ETA: 14s - loss: 1.3882 - accuracy: 0.3281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-03 22:56:46.558418: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 85ms/step - loss: 1.2168 - accuracy: 0.5273\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.8627 - accuracy: 0.6689\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.7865 - accuracy: 0.7075\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.7469 - accuracy: 0.7324\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.7370 - accuracy: 0.7246\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.6697 - accuracy: 0.7622\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.7081 - accuracy: 0.7378\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.6761 - accuracy: 0.7461\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.6617 - accuracy: 0.7563\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.6435 - accuracy: 0.7524\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.6197 - accuracy: 0.7720\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5880 - accuracy: 0.7803\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.6426 - accuracy: 0.7617\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5774 - accuracy: 0.7866\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.6085 - accuracy: 0.7793\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.5698 - accuracy: 0.7954\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.5526 - accuracy: 0.7988\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5937 - accuracy: 0.7920\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5527 - accuracy: 0.8013\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5433 - accuracy: 0.7998\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5512 - accuracy: 0.8086\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4926 - accuracy: 0.8232\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5346 - accuracy: 0.7979\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5157 - accuracy: 0.8174\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5065 - accuracy: 0.8291\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5172 - accuracy: 0.8164\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4815 - accuracy: 0.8325\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4889 - accuracy: 0.8340\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4976 - accuracy: 0.8198\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.4565 - accuracy: 0.8379\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4853 - accuracy: 0.8311\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.5015 - accuracy: 0.8237\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4795 - accuracy: 0.8315\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4436 - accuracy: 0.8423\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4560 - accuracy: 0.8438\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.4667 - accuracy: 0.8354\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4638 - accuracy: 0.8379\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4266 - accuracy: 0.8481\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.4416 - accuracy: 0.8511\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4357 - accuracy: 0.8530\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.4379 - accuracy: 0.8516\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4227 - accuracy: 0.8516\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4424 - accuracy: 0.8506\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3973 - accuracy: 0.8594\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4096 - accuracy: 0.8638\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.4002 - accuracy: 0.8657\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.4346 - accuracy: 0.8472\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4187 - accuracy: 0.8501\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.4114 - accuracy: 0.8574\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3848 - accuracy: 0.8701\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3843 - accuracy: 0.8765\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3849 - accuracy: 0.8657\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3970 - accuracy: 0.8579\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3754 - accuracy: 0.8682\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3779 - accuracy: 0.8730\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3599 - accuracy: 0.8779\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3565 - accuracy: 0.8804\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3656 - accuracy: 0.8701\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3493 - accuracy: 0.8862\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3516 - accuracy: 0.8828\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.4182 - accuracy: 0.8623\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3854 - accuracy: 0.8755\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3372 - accuracy: 0.8901\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3520 - accuracy: 0.8838\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3740 - accuracy: 0.8696\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3563 - accuracy: 0.8770\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3250 - accuracy: 0.8862\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3335 - accuracy: 0.8872\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3040 - accuracy: 0.9053\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3288 - accuracy: 0.8901\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3415 - accuracy: 0.8833\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2959 - accuracy: 0.8955\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.3052 - accuracy: 0.8965\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3271 - accuracy: 0.8833\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2724 - accuracy: 0.9067\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2846 - accuracy: 0.9038\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2993 - accuracy: 0.8960\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2802 - accuracy: 0.9067\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2933 - accuracy: 0.8989\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2935 - accuracy: 0.8984\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3023 - accuracy: 0.9038\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2738 - accuracy: 0.9097\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2696 - accuracy: 0.9131\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.3060 - accuracy: 0.8936\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2918 - accuracy: 0.8989\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2691 - accuracy: 0.9062\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2658 - accuracy: 0.9136\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2896 - accuracy: 0.9102\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2919 - accuracy: 0.9023\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2721 - accuracy: 0.9092\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2540 - accuracy: 0.9102\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2633 - accuracy: 0.9155\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2089 - accuracy: 0.9355\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2186 - accuracy: 0.9321\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2905 - accuracy: 0.8999\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.2395 - accuracy: 0.9155\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2589 - accuracy: 0.9111\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2625 - accuracy: 0.9136\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2675 - accuracy: 0.9131\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.2163 - accuracy: 0.9292\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    X, y = data.batch(2048)\n",
    "    model.fit(X, y, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f82120-21b7-4baa-af74-0cd9c972ba37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
