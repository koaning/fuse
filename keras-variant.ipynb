{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f6e29d7-0fb5-4cbe-9a14-ae5e349969d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy.sparse import vstack\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tokenwiser.pipeline import make_partial_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9a1fc56-ee02-4687-a0d8-4c2e0c8e88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset:\n",
    "    def __init__(self, path, text_col='text', label_col='label'):\n",
    "        dataf = pd.read_csv(path)\n",
    "        self.train = dataf.loc[lambda d: d['split'] == 'train'].reset_index()\n",
    "        self.valid = dataf.loc[lambda d: d['split'] == 'valid'].reset_index()\n",
    "        self.labels = list(dataf[label_col].unique())\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.name = path + (\"\" if label_col == \"label\" else \"-\" + label_col)\n",
    "        label_arr = np.array(self.labels).reshape(-1, 1)\n",
    "        self.label_enc = OneHotEncoder(sparse=False).fit(label_arr)\n",
    "    \n",
    "    def batch(self, n):\n",
    "        \"\"\"Generates (texts, labels) datasets\"\"\"\n",
    "        indices = np.random.randint(len(self.train), size=n)\n",
    "        subset = self.train.iloc[indices]\n",
    "        label_arr = np.array(subset[self.label_col]).reshape(-1, 1)\n",
    "        return subset[self.text_col], self.label_enc.transform(label_arr)\n",
    "    \n",
    "    def full(self, split=\"train\"):\n",
    "        subset = self.train if split == \"train\" else self.valid\n",
    "        label_arr = np.array(subset[self.label_col]).reshape(-1, 1)\n",
    "        return subset[self.text_col], self.label_enc.transform(label_arr)\n",
    "\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, dataset, tokeniser):\n",
    "        self.dataset = dataset\n",
    "        self.tokeniser = tokeniser\n",
    "        label_arr = np.array(self.dataset.labels).reshape(-1, 1)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<Batcher {self.dataset.name} n={self.dataset.train.shape[0]} k={len(self.dataset.labels)}>\"\n",
    "    \n",
    "    def batch(self, n):\n",
    "        text, labs = self.dataset.batch(n=n)\n",
    "        return self.tokeniser.transform(text), labs\n",
    "    \n",
    "    def batch_X_s_y(self, n):\n",
    "        \"\"\"Generates (texts, label_ids, similarity) datasets\"\"\"\n",
    "        X, y = self.batch(n)\n",
    "        X_out, s_out, y_out = [], [], []\n",
    "        for row_idx, text in enumerate(X):\n",
    "            for idx, lab in enumerate(y[row_idx]):\n",
    "                X_out.append(text)\n",
    "                s_out.append([0 if i != idx else 1 for i in range(y.shape[1])])\n",
    "                y_out.append(lab)\n",
    "        return vstack(X_out), np.array(s_out), np.array(y_out)\n",
    "    \n",
    "    def full(self, split=\"train\"):\n",
    "        text, labs = self.dataset.full(split=split)\n",
    "        return self.tokeniser.transform(text), labs\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.tokeniser.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b49d1266-68f3-4efa-93cc-32efb39cb5d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, name='silicone', subset='dyda_da', split='train', n_feat=5_000):\n",
    "#         self.dataset = load_dataset(name, subset)\n",
    "#         if isinstance(self.dataset, DatasetDict):\n",
    "#             self.dataset = self.dataset[split]\n",
    "#         self.labels = list(set(i['Label'] for i in self.dataset))\n",
    "#         self.name = f\"{name}-{subset}-{split}\"\n",
    "#         self.tfm = make_partial_union(\n",
    "#             HashingVectorizer(n_features=n_feat),\n",
    "#             HashingVectorizer(n_features=n_feat, ngram_range=(2, 2)),\n",
    "#         )\n",
    "#         self.label_enc = OneHotEncoder(sparse=False).fit(np.array(self.labels).reshape(-1, 1))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.dataset[idx]\n",
    "#         return item['Utterance'], item['Label']\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return f\"<TextDataset {self.name}>\"\n",
    "    \n",
    "#     def batch_X_y(self, n):\n",
    "#         \"\"\"Samples a random batch of `n` datapoints.\"\"\"\n",
    "#         indices = np.random.randint(len(self), size=n)\n",
    "#         texts, labels = zip(*[self[int(i)] for i in indices])\n",
    "#         X = self.tfm.transform(texts)\n",
    "#         y = self.label_enc.transform(np.array(labels).reshape(-1, 1))\n",
    "#         return X, y\n",
    "    \n",
    "#     def batch_X_s_y(self, n):\n",
    "#         X, y = self.batch_X_y(n)\n",
    "#         res = []\n",
    "#         for text in X:\n",
    "#             for idx, lab in enumerate(y):\n",
    "#                 res.append(text, idx, np.argmax(lab))\n",
    "#         return res\n",
    "    \n",
    "#     def full(self):\n",
    "#         \"\"\"Returns the full set in matrix form.\"\"\"\n",
    "#         texts, labels = zip(*[self[int(i)] for i in range(len(self))])\n",
    "#         X = self.tfm.transform(texts)\n",
    "#         y = self.label_enc.transform(np.array(labels).reshape(-1, 1))\n",
    "#         return X, y\n",
    "    \n",
    "#     def transform(self, texts):\n",
    "#         return self.tfm.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f759162-5c00-4b72-9995-9d5e8320ba43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22374                    what do you mean by that ?\n",
       " 74382    i'm here to see about a fixed asset loan .\n",
       " Name: text, dtype: object,\n",
       " array([[0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClassificationDataset(\"data/silicone-dyda_da.csv\").batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41537dc7-b317-4fd5-8415-9cec9422443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = 2000\n",
    "\n",
    "tok = tfm = make_partial_union(\n",
    "            HashingVectorizer(n_features=n_feat),\n",
    "            HashingVectorizer(n_features=n_feat, ngram_range=(2, 2)),\n",
    "        )\n",
    "    \n",
    "my_datasets = {d.name: {'dataset': Batcher(d, tokeniser=tok)} for d in [\n",
    "    ClassificationDataset(\"data/silicone-dyda_da.csv\"),\n",
    "    ClassificationDataset(\"data/silicone-dyda_e.csv\"), \n",
    "    ClassificationDataset(\"data/silicone-meld_e.csv\"),\n",
    "    ClassificationDataset(\"data/tweet_eval-emoji.csv\"),\n",
    "    ClassificationDataset(\"data/tweet_eval-emotion.csv\"),\n",
    "    ClassificationDataset(\"data/tweet_eval-sentiment.csv\"),\n",
    "    ClassificationDataset(\"data/google-emotions.csv\", label_col=\"anger\"),\n",
    "]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456dbdd-71f5-42b1-9992-d066e926adb4",
   "metadata": {},
   "source": [
    "```python\n",
    "fuse = (\n",
    "    FUSE(tokeniser, n_tok_feat)\n",
    "      .add_task(name, subset)\n",
    "      .add_task(name, subset)\n",
    "      .add_task(name, subset)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c71c5cdf-4a1b-48fe-890f-64411faec3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/silicone-dyda_da.csv': {'dataset': <Batcher data/silicone-dyda_da.csv n=87170 k=4>},\n",
       " 'data/silicone-dyda_e.csv': {'dataset': <Batcher data/silicone-dyda_e.csv n=87170 k=7>},\n",
       " 'data/silicone-meld_e.csv': {'dataset': <Batcher data/silicone-meld_e.csv n=9989 k=7>},\n",
       " 'data/tweet_eval-emoji.csv': {'dataset': <Batcher data/tweet_eval-emoji.csv n=45000 k=20>},\n",
       " 'data/tweet_eval-emotion.csv': {'dataset': <Batcher data/tweet_eval-emotion.csv n=3257 k=4>},\n",
       " 'data/tweet_eval-sentiment.csv': {'dataset': <Batcher data/tweet_eval-sentiment.csv n=45615 k=3>},\n",
       " 'data/google-emotions.csv-anger': {'dataset': <Batcher data/google-emotions.csv-anger n=169046 k=2>}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a8d689b-e542-48b8-9764-18c32d6181e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, Embedding, Dot\n",
    "from keras.models import Model\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "X = tok.transform([\"hello\"])\n",
    "emb_dim = 256\n",
    "inputs = Input(shape=(X.shape[1],), sparse=True, name=\"text_input\")\n",
    "emb1 = Dense(emb_dim, activation='relu', name=\"dense_layer_1\")(inputs)\n",
    "emb2 = Dense(emb_dim, activation='relu', name=\"dense_layer_2\")(emb1)\n",
    "\n",
    "for dataset in my_datasets.values():\n",
    "    X, s, y = dataset['dataset'].batch_X_s_y(8)\n",
    "    \n",
    "    label_shape = len(dataset['dataset'].dataset.labels)\n",
    "    label_input = Input(shape=(s.shape[1],), name=\"label_input\")\n",
    "    label_emb   = Dense(emb_dim, name=\"label_emb\")(label_input)\n",
    "    dot_prod    = Dot(axes=(1,1), name=\"dot_product\")([label_emb, emb2])\n",
    "    output      = Dense(1, activation='sigmoid', name=\"task_output\")(dot_prod)\n",
    "    \n",
    "    dataset['outputs'] = output\n",
    "    dataset['model'] = Model(inputs=[inputs, label_input], outputs=dataset['outputs'])\n",
    "    dataset['model'].compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee0b7884-eac3-4ae9-9557-3a9f0e9ccfbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/silicone-dyda_da.csv\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3483 - accuracy: 0.8535\n",
      "data/silicone-dyda_e.csv\n",
      "224/224 [==============================] - 2s 8ms/step - loss: 0.1224 - accuracy: 0.9583\n",
      "data/silicone-meld_e.csv\n",
      "224/224 [==============================] - 2s 7ms/step - loss: 0.2781 - accuracy: 0.8911\n",
      "data/tweet_eval-emoji.csv\n",
      "640/640 [==============================] - 5s 8ms/step - loss: 0.1869 - accuracy: 0.9500\n",
      "data/tweet_eval-emotion.csv\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.2315 - accuracy: 0.9154\n",
      "data/tweet_eval-sentiment.csv\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.5578 - accuracy: 0.7113\n",
      "data/google-emotions.csv-anger\n",
      "64/64 [==============================] - 1s 8ms/step - loss: 0.1672 - accuracy: 0.9583\n",
      "data/silicone-dyda_da.csv\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3492 - accuracy: 0.8550\n",
      "data/silicone-dyda_e.csv\n",
      "224/224 [==============================] - 2s 8ms/step - loss: 0.1244 - accuracy: 0.9592\n",
      "data/silicone-meld_e.csv\n",
      "224/224 [==============================] - 2s 7ms/step - loss: 0.2581 - accuracy: 0.9009\n",
      "data/tweet_eval-emoji.csv\n",
      "640/640 [==============================] - 5s 8ms/step - loss: 0.1792 - accuracy: 0.9503\n",
      "data/tweet_eval-emotion.csv\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.1273 - accuracy: 0.9587\n",
      "data/tweet_eval-sentiment.csv\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.5193 - accuracy: 0.7358\n",
      "data/google-emotions.csv-anger\n",
      "64/64 [==============================] - 1s 8ms/step - loss: 0.1529 - accuracy: 0.9595\n",
      "data/silicone-dyda_da.csv\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3236 - accuracy: 0.8658\n",
      "data/silicone-dyda_e.csv\n",
      "224/224 [==============================] - 2s 8ms/step - loss: 0.1227 - accuracy: 0.9570\n",
      "data/silicone-meld_e.csv\n",
      "224/224 [==============================] - 2s 7ms/step - loss: 0.2345 - accuracy: 0.9114\n",
      "data/tweet_eval-emoji.csv\n",
      "640/640 [==============================] - 5s 8ms/step - loss: 0.1748 - accuracy: 0.9505\n",
      "data/tweet_eval-emotion.csv\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.0771 - accuracy: 0.9777\n",
      "data/tweet_eval-sentiment.csv\n",
      " 7/96 [=>............................] - ETA: 0s - loss: 0.5334 - accuracy: 0.7288"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1565659/3765422073.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                           class_weight=dict(enumerate(weights)))\n\u001b[0m",
      "\u001b[0;32m~/Development/fusebox/venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/fusebox/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/fusebox/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/fusebox/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/fusebox/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/fusebox/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Development/fusebox/venv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    for dataset in my_datasets.keys():\n",
    "        X, s, y = my_datasets[dataset]['dataset'].batch_X_s_y(2048*2)\n",
    "        print(dataset)\n",
    "        valid_data = my_datasets[dataset]['dataset'].full(split=\"valid\")\n",
    "        labels = sorted(my_datasets[dataset]['dataset'].dataset.labels)\n",
    "        weights = class_weight.compute_class_weight('balanced', classes=labels, y=np.argmax(s, axis=1))\n",
    "        my_datasets[dataset]['model'].fit(x=[X, s], \n",
    "                                          y=y, \n",
    "                                          batch_size=128, \n",
    "                                          epochs=1, \n",
    "                                          class_weight=dict(enumerate(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075b583-27bc-4374-8401-a1bfc435a101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06f82120-21b7-4baa-af74-0cd9c972ba37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.623198</td>\n",
       "      <td>0.081766</td>\n",
       "      <td>0.138506</td>\n",
       "      <td>0.005683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.623198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.040192</td>\n",
       "      <td>0.036020</td>\n",
       "      <td>-0.033577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081766</td>\n",
       "      <td>-0.040192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830557</td>\n",
       "      <td>0.932096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.138506</td>\n",
       "      <td>0.036020</td>\n",
       "      <td>0.830557</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005683</td>\n",
       "      <td>-0.033577</td>\n",
       "      <td>0.932096</td>\n",
       "      <td>0.772061</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  1.000000  0.623198  0.081766  0.138506  0.005683\n",
       "1  0.623198  1.000000 -0.040192  0.036020 -0.033577\n",
       "2  0.081766 -0.040192  1.000000  0.830557  0.932096\n",
       "3  0.138506  0.036020  0.830557  1.000000  0.772061\n",
       "4  0.005683 -0.033577  0.932096  0.772061  1.000000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = Model(inputs=inputs, outputs=emb2)\n",
    "pd.DataFrame(emb_model.predict(tok.transform([\"bad\", \"evil\", \"good\", \"joy\", \"happy\"]))).T.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56795308-1be0-4d08-9752-32dd13384a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
